{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4253 / 5253 - Lab #4 - Patent Problem with Spark RDD - SOLUTION\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Spark cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/06 15:34:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/06 15:34:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf=SparkConf().setAppName(\"Lab4-rdd\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PySpark and RDD's on the https://coding.csel.io machines is slow -- most of the code is executed in Python and this is much less efficient than the java-based code using the PySpark dataframes. Be patient and trying using `.cache()` to cache the output of joins. You may want to start with a reduced set of data before running the full task. You can use the `sample()` method to extract just a sample of the data or use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two RDD's are called \"rawCitations\" and \"rawPatents\" because you probably want to process them futher (e.g. convert them to integer types, etc). \n",
    "\n",
    "The `textFile` function returns data in strings. This should work fine for this lab.\n",
    "\n",
    "Other methods you use might return data in type `Byte`. If you haven't used Python `Byte` types before, google it. You can convert a value of `x` type byte into e.g. a UTF8 string using `x.decode('uft-8')`. Alternatively, you can use the `open` method of the gzip library to read in all the lines as UTF-8 strings like this:\n",
    "```\n",
    "import gzip\n",
    "with gzip.open('cite75_99.txt.gz', 'rt',encoding='utf-8') as f:\n",
    "    rddCitations = sc.parallelize( f.readlines() )\n",
    "```\n",
    "This is less efficient than using `textFile` because `textFile` would use the underlying HDFS or other file system to read the file across all the worker nodes while the using `gzip.open()...readlines()` will read all the data in the frontend and then distribute it to all the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCitations = sc.textFile(\"cite75_99.txt.gz\")\n",
    "rddPatents = sc.textFile(\"apat63_99.txt.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\"CITING\",\"CITED\"',\n",
       " '3858241,956203',\n",
       " '3858241,1324234',\n",
       " '3858241,3398406',\n",
       " '3858241,3557384']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCitations.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\"PATENT\",\"GYEAR\",\"GDATE\",\"APPYEAR\",\"COUNTRY\",\"POSTATE\",\"ASSIGNEE\",\"ASSCODE\",\"CLAIMS\",\"NCLASS\",\"CAT\",\"SUBCAT\",\"CMADE\",\"CRECEIVE\",\"RATIOCIT\",\"GENERAL\",\"ORIGINAL\",\"FWDAPLAG\",\"BCKGTLAG\",\"SELFCTUB\",\"SELFCTLB\",\"SECDUPBD\",\"SECDLWBD\"',\n",
       " '3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,',\n",
       " '3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,',\n",
       " '3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,',\n",
       " '3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddPatents.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, they are a single string with multiple CSV's. You will need to convert these to (K,V) pairs, probably convert the keys to `int` and so on. You'll need to `filter` out the header string as well since there's no easy way to extract all the lines except the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "patentHeader = rddPatents.first()\n",
    "\n",
    "\n",
    "citationHeader = rddCitations.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddPatentsNoHeader = rddPatents.filter(lambda x: x != patentHeader).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,',\n",
       " '3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,',\n",
       " '3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,',\n",
       " '3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,',\n",
       " '3070805,1963,1096,,\"US\",\"CA\",,1,,2,6,63,,1,,0,,,,,,,']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddPatentsNoHeader.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "patentHeader = patentHeader.split(\",\")\n",
    "patentHeader = [x.strip(\"\\\"\") for x in patentHeader]\n",
    "\n",
    "citationHeader = citationHeader.split(\",\")\n",
    "citationHeader = [x.strip(\"\\\"\") for x in citationHeader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddPatentsSplit = rddPatentsNoHeader.map(lambda x: (int(x.split(\",\")[0].strip()), {patentHeader[i+1]: int(val.strip()) if val.strip().isdigit() else val.strip(\"\\\"\") for i, val in enumerate(x.split(\",\")[1:])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddPatentState = rddPatentsSplit.map(lambda x: (x[0], x[1]['POSTATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddPatentState = rddPatentState.filter(lambda x: x[1] != \"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3070802, 'TX'),\n",
       " (3070803, 'IL'),\n",
       " (3070804, 'OH'),\n",
       " (3070805, 'CA'),\n",
       " (3070806, 'PA')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddPatentState.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'CITING': 'CITING', 'CITED': 'CITED'},\n",
       " {'CITING': 3858241, 'CITED': 956203},\n",
       " {'CITING': 3858241, 'CITED': 1324234},\n",
       " {'CITING': 3858241, 'CITED': 3398406},\n",
       " {'CITING': 3858241, 'CITED': 3557384}]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rCitationsNoHeader = rddCitations.filter(lambda x: x != citationHeader)\n",
    "rCitationsSplit = rCitationsNoHeader.map(lambda x: {citationHeader[i]: int(val.strip()) if val.strip().isdigit() else val.strip(\"\\\"\") for i, val in enumerate(x.split(\",\"))}).cache()\n",
    "\n",
    "rCitationsDict = rCitationsSplit.map(lambda x: {key: (None if value == \"\" else value) for key, value in x.items()}).cache()\n",
    "rCitationsDict.take(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCiting = rCitationsSplit.map(lambda x: (x['CITING'], x)).cache()\n",
    "rddCited = rCitationsSplit.map(lambda x: (x['CITED'], x)).cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCitationsState = rddCiting.leftOuterJoin(rddPatentState).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCitingWithState = rddCitationsState.map(lambda x: (\n",
    "    x[0],  # CITING\n",
    "    (x[1][0]['CITED'], x[1][1])  # (CITED, CITING_STATE)\n",
    ")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCitedState = rddCited.leftOuterJoin(rddPatentState).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCitedStateKV = rddCitedState.map(lambda x: (\n",
    "    x[1][0]['CITED'],  # CITED from citation dict\n",
    "    x[1][1]  # CITED_STATE\n",
    ")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddWithBothStates = rddCitingWithState.map(lambda x: (x[1][0], (x[0], x[1][1]))).join(rddCitedStateKV).cache()\n",
    "# (CITING, ((CITED, CITING_STATE), CITED_STATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 17:09:42 WARN MemoryStore: Not enough space to cache rdd_190_0 in memory! (computed 93.4 MiB so far)\n",
      "26/02/06 17:09:42 WARN BlockManager: Putting block rdd_190_0 failed\n",
      "26/02/06 17:09:42 WARN BlockManager: Block rdd_190_0 could not be removed as it was not found on disk or in memory\n",
      "26/02/06 17:09:45 WARN MemoryStore: Not enough space to cache rdd_191_0 in memory! (computed 176.7 MiB so far)\n",
      "26/02/06 17:09:45 WARN BlockManager: Putting block rdd_191_0 failed\n",
      "26/02/06 17:09:45 WARN BlockManager: Block rdd_191_0 could not be removed as it was not found on disk or in memory\n",
      "26/02/06 17:12:17 WARN MemoryStore: Not enough space to cache rdd_200_0 in memory! (computed 19.0 MiB so far)\n",
      "26/02/06 17:12:17 WARN BlockManager: Putting block rdd_200_0 failed\n",
      "26/02/06 17:12:17 WARN BlockManager: Block rdd_200_0 could not be removed as it was not found on disk or in memory\n",
      "26/02/06 17:12:18 WARN MemoryStore: Not enough space to cache rdd_200_1 in memory! (computed 28.9 MiB so far)\n",
      "26/02/06 17:12:18 WARN BlockManager: Putting block rdd_200_1 failed\n",
      "26/02/06 17:12:18 WARN BlockManager: Block rdd_200_1 could not be removed as it was not found on disk or in memory\n",
      "26/02/06 17:12:23 WARN MemoryStore: Not enough space to cache rdd_208_0 in memory! (computed 97.1 MiB so far)\n",
      "26/02/06 17:12:23 WARN BlockManager: Putting block rdd_208_0 failed\n",
      "26/02/06 17:12:23 WARN BlockManager: Block rdd_208_0 could not be removed as it was not found on disk or in memory\n",
      "26/02/06 17:12:25 WARN MemoryStore: Not enough space to cache rdd_208_1 in memory! (computed 96.5 MiB so far)\n",
      "26/02/06 17:12:25 WARN BlockManager: Putting block rdd_208_1 failed\n",
      "26/02/06 17:12:25 WARN BlockManager: Block rdd_208_1 could not be removed as it was not found on disk or in memory\n",
      "26/02/06 17:12:25 WARN MemoryStore: Not enough space to cache rdd_209_0 in memory! (computed 5.0 MiB so far)\n",
      "26/02/06 17:12:25 WARN BlockManager: Putting block rdd_209_0 failed\n",
      "26/02/06 17:12:25 WARN BlockManager: Block rdd_209_0 could not be removed as it was not found on disk or in memory\n",
      "26/02/06 17:12:25 WARN MemoryStore: Not enough space to cache rdd_209_1 in memory! (computed 1269.1 KiB so far)\n",
      "26/02/06 17:12:25 WARN BlockManager: Putting block rdd_209_1 failed\n",
      "26/02/06 17:12:25 WARN BlockManager: Block rdd_209_1 could not be removed as it was not found on disk or in memory\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrddWithBothStates\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py:2722\u001b[39m, in \u001b[36mRDD.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m   2719\u001b[39m         taken += \u001b[32m1\u001b[39m\n\u001b[32m   2721\u001b[39m p = \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned + numPartsToTry, totalParts))\n\u001b[32m-> \u001b[39m\u001b[32m2722\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2724\u001b[39m items += res\n\u001b[32m   2725\u001b[39m partsScanned += numPartsToTry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/context.py:2570\u001b[39m, in \u001b[36mSparkContext.runJob\u001b[39m\u001b[34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[39m\n\u001b[32m   2568\u001b[39m mappedRDD = rdd.mapPartitions(partitionFunc)\n\u001b[32m   2569\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2570\u001b[39m sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/py4j/clientserver.py:535\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         answer = smart_decode(\u001b[38;5;28mself\u001b[39m.stream.readline()[:-\u001b[32m1\u001b[39m])\n\u001b[32m    536\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    537\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    538\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 17:13:21 WARN BlockManager: Putting block rdd_217_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 3386, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 3375, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 705, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 4053, in groupByKey\n",
      "    merger.mergeCombiners(it)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 776, in _spill\n",
      "    self.serializer.dump_stream([(k, v)], streams[h])\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 277, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 539, in dumps\n",
      "    return zlib.compress(self.serializer.dumps(obj), 1)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 460, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 1543, in dumps\n",
      "    cp = Pickler(file, protocol=protocol, buffer_callback=buffer_callback)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 1321, in __init__\n",
      "    super().__init__(file, protocol=protocol, buffer_callback=buffer_callback)\n",
      "KeyboardInterrupt\n",
      ".\n",
      "26/02/06 17:13:21 WARN BlockManager: Block rdd_217_0 could not be removed as it was not found on disk or in memory\n",
      "26/02/06 17:13:21 ERROR Executor: Exception in task 0.0 in stage 75.0 (TID 81)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 3386, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 3375, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 705, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 4053, in groupByKey\n",
      "    merger.mergeCombiners(it)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 776, in _spill\n",
      "    self.serializer.dump_stream([(k, v)], streams[h])\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 277, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 539, in dumps\n",
      "    return zlib.compress(self.serializer.dumps(obj), 1)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 460, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 1543, in dumps\n",
      "    cp = Pickler(file, protocol=protocol, buffer_callback=buffer_callback)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 1321, in __init__\n",
      "    super().__init__(file, protocol=protocol, buffer_callback=buffer_callback)\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:368)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1676)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "26/02/06 17:13:21 WARN TaskSetManager: Lost task 0.0 in stage 75.0 (TID 81) (cu-genvpn-tcom-10.180.165.145.int.colorado.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 3386, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 3375, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 5306, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 705, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/core/rdd.py\", line 4053, in groupByKey\n",
      "    merger.mergeCombiners(it)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 776, in _spill\n",
      "    self.serializer.dump_stream([(k, v)], streams[h])\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 277, in dump_stream\n",
      "    bytes = self.serializer.dumps(vs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 539, in dumps\n",
      "    return zlib.compress(self.serializer.dumps(obj), 1)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 460, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 1543, in dumps\n",
      "    cp = Pickler(file, protocol=protocol, buffer_callback=buffer_callback)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/lab4/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 1321, in __init__\n",
      "    super().__init__(file, protocol=protocol, buffer_callback=buffer_callback)\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:230)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:368)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1676)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1585)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1650)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1429)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1383)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "26/02/06 17:13:21 ERROR TaskSetManager: Task 0 in stage 75.0 failed 1 times; aborting job\n"
     ]
    }
   ],
   "source": [
    "rddWithBothStates.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddWithBothStatesSame = rddWithBothStates.filter(lambda x: x[1][1] == x[1][0][1]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3707004, ((3858242, None), None)),\n",
       " (3707004, ((3858242, None), None)),\n",
       " (3707004, ((3858242, None), None)),\n",
       " (3707004, ((3978525, None), None)),\n",
       " (3707004, ((3978525, None), None))]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddWithBothStatesSame.groupBy(lambda x: x[1][0]).agg(count(\"CITING\").alias(\"CITATION_COUNT\")).orderBy(col(\"CITATION_COUNT\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
